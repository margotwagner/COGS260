{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T22:44:09.212289Z",
     "start_time": "2020-05-18T22:44:08.992779Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:25:33.587077Z",
     "start_time": "2020-05-20T01:25:33.584087Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Load in data and return values\n",
    "    \n",
    "    param:   filename (str) - filename/path to data\n",
    "    return:  data - data values\n",
    "    '''\n",
    "    # double check it's samples x features\n",
    "    data = pd.read_csv(filename)\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:27:40.424920Z",
     "start_time": "2020-05-20T01:27:40.421587Z"
    }
   },
   "outputs": [],
   "source": [
    "def norm_data(data):\n",
    "    '''\n",
    "    Normalize data to z-values (0 mean and 1 std dev)\n",
    "    \n",
    "    param:   data - data values (n_samples, n_features)\n",
    "    return:  data - normalized data\n",
    "    '''\n",
    "    means = data.mean(axis=0)    # mean for each feature\n",
    "    stdevs = data.std(axis=0)    # std dev for each feature\n",
    "    data = (data - means) / stdevs    # normalized data (Z-score)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:11:36.991443Z",
     "start_time": "2020-05-20T02:11:36.986913Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca(data):\n",
    "    '''\n",
    "    Does PCA on normalized data set. Can optionally set fewer components\n",
    "    \n",
    "    param: data - normalized data (n_samples, n_features)\n",
    "    return: \n",
    "        data_pc: data transformed onto princinpal components\n",
    "        components:  principal axes in feature space, array, shape (n_components, n_features)\n",
    "        weights: percentage of variance explained by each of the selected components. array, shape (n_components,)\n",
    "    '''\n",
    "    # create PCA model\n",
    "    pca = PCA() \n",
    "    \n",
    "    # fit model to data\n",
    "    data_pc = pca.fit(data)  \n",
    "    \n",
    "    # obtain components and components' weights\n",
    "    components = pca.components_\n",
    "    weights = pca.explained_variance_ratio_\n",
    "    \n",
    "    return data_pc, components, weights, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:09:37.149011Z",
     "start_time": "2020-05-20T02:09:37.143953Z"
    }
   },
   "outputs": [],
   "source": [
    "def cum_var_plot(weights, desired_var):\n",
    "    '''\n",
    "    Cumulative variance plot (number of components vs cumulative variance captured) with calculated number \n",
    "    of PCs required to get to a certain desired variance explained\n",
    "    \n",
    "    params:\n",
    "        weights: percentage of variance explained by each of the selected components. array, shape (n_components,)\n",
    "        desired_var:  percent variance to find number of PCs for\n",
    "    return\n",
    "        pcs_req:   pcs required to captured at least desired variance\n",
    "        captured_var   exact variance captured by pcs_req\n",
    "    \n",
    "    '''\n",
    "    INDEX_SHIFT = 1\n",
    "    # cumulative variance captured\n",
    "    cum_var = np.cumsum(weights) \n",
    "    \n",
    "    # find pcs req to get desired variance\n",
    "    pcs_req = ceil(np.min(np.where(cum_var > desired_var)))   \n",
    "    \n",
    "    # actual variance captured\n",
    "    captured_var = cumulative_var[pcs_req-INDEX_SHIFT]\n",
    "    \n",
    "    # plotting\n",
    "    plt.plot(range(INDEX_SHIFT,len(cum_var)+INDEX_SHIFT), cum_var)\n",
    "    plt.axvline(x=pcs_req, ymin=0, ymax=1, color='k', linestyle='--')\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative variance captured')\n",
    "    plt.title('Cumulative Variance Captured by Principal Components')\n",
    "    \n",
    "    return pcs_req, captured_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(data, dim=2, pca):\n",
    "    '''\n",
    "    Creates biplot for data mapping data onto top 2 or 3 principal components\n",
    "    \n",
    "    params\n",
    "        data - normalized data, array shape (n_samples, n_features)\n",
    "        dim - 2 or 3 dimensions for plotting\n",
    "        pca - pca model\n",
    "    '''\n",
    "    PC1_IDX = 0\n",
    "    PC2_IDX = 1\n",
    "    \n",
    "    # top 2/3 pc's and their variance explainted\n",
    "    top_pcs = pca.transform(data)[:,:dim]\n",
    "    top_var = pca.explained_variance_ratio_[:dim]\n",
    "    \n",
    "    # plotting\n",
    "    plt.scatter(top_pcs[:,PC1_IDX], top_pcs[:,PC2_IDX])\n",
    "    plt.xlabel('PC1 ({.1%})'.format(top_pcs[PC1_IDX]))\n",
    "    plt.ylabel('PC2 ({.1%})'.format(top_pcs[PC2_IDX]))\n",
    "    plt.axvline(x=0, ymin=np.min(top_pcs[:,PC2_IDX]), ymax=np.max(top_pcs[:,PC2_IDX]), 'k--')\n",
    "    plt.axhline(y=0, xmin=np.min(top_pcs[:,PC1_IDX]), xmax=np.max(top_pcs[:,PC1_IDX]), 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def lda(data):\n",
    "    '''\n",
    "    Linear discriminant analysis (LDA) classifier for input data into 3 stimulus conditions (left, right, no stimulus)\n",
    "    \n",
    "    param:\n",
    "        data - normalized and potentially transformed to pcs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    N_TRIALS = 20\n",
    "    labels = np.array(['left', 'right', 'none'])\n",
    "    labels = np.repeat(labels, N_TRIALS)\n",
    "    \n",
    "    # TRANSFORM DATA TO LOWER DIM\n",
    "    # Automatically sets test as 0.25\n",
    "    # Random state so that it's the same every run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "    # train LDA classifier\n",
    "    # look into using LDA for dimensionality reduction??\n",
    "    lda_model = LinearDiscriminantAnalysis.fit(X_train , y_train)\n",
    "    y_predict = lda_model.predict(X_test)\n",
    "    \n",
    "    # model accuracy for X_test\n",
    "    lda_acc = 100*accuracy_score(y_test, y_predict)\n",
    "    print('Accuracy:',round(lda_acc,2),'%')\n",
    "    print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "    \n",
    "    # creating a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves have true positive (predicted positive and actually positive) rate on the Y axis and false positive (predicted positive, actually negative) on the X axis. THe top left corner for the plot is the \"ideal\" point with a false positive rate of zero and a true positive rate of one.  \n",
    "  \n",
    "In order to extend ROC curve and ROC area to multi-label classification, it is necessary to binarize the output (one vs all). One ROC curve can be drawn per label and/or one can draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).  \n",
    "  \n",
    "Another evaluation for multi-label classification is macro-averaging, which gives equal weight to the classification of each label.  \n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#plot-roc-curves-for-the-multilabel-problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve():\n",
    "    #https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "    #https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier/56092736\n",
    "    #https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#in-multi-label-settings\n",
    "    \n",
    "    # Precision-Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Add regularization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Add bootstrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
