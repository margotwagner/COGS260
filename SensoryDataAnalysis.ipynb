{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created 5.19.20\n",
    "\n",
    "Authors: Margot Wagner, Sam Russman\n",
    "\n",
    "COGS 260 Neural Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:03:03.564283Z",
     "start_time": "2020-05-28T20:03:02.387853Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from math import ceil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data as matrix \"X\" of samples x features\n",
    "- X starts as [n_chans x n_tp x n_samples x n_conditions] = [360 x 51 x 21 x 4]\n",
    "- In this case we are splitting the baselines into two separate conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:22:17.294069Z",
     "start_time": "2020-05-28T20:22:17.288260Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename, transpose=False):\n",
    "    '''\n",
    "    Load in data and return values\n",
    "    \n",
    "    param:   filename (str) - filename/path to data\n",
    "             transpose - transposes data if true (default False)\n",
    "    return:  data - data values (n_samples, n_features) \n",
    "    '''\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    \n",
    "    # needs to be samples x features\n",
    "    if transpose:    \n",
    "        data = data.transpose()\n",
    "        \n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:22:18.907586Z",
     "start_time": "2020-05-28T20:22:17.620323Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data, X: (360, 51, 21, 4)\n"
     ]
    }
   ],
   "source": [
    "'''Create X matrix'''\n",
    "N_CHANS = 360\n",
    "N_TIMEPTS = 51\n",
    "N_SAMPLES = 21\n",
    "N_CONDITIONS = 4\n",
    "X = np.zeros((N_CHANS, N_TIMEPTS, N_SAMPLES, N_CONDITIONS))\n",
    "L_COND_IDX = 0\n",
    "R_COND_IDX = 1\n",
    "NO_COND_IDX_1 = 2   # first half of baseline samples\n",
    "NO_COND_IDX_2 = 3   # second half of baselines samples\n",
    "IDX_SHIFT = 1       # index adjustments due to zero indexing\n",
    "\n",
    "# 60mA stim and baseline\n",
    "os.chdir('/Volumes/GoogleDrive/Shared drives/COGS 260 Project/Data/fragments3')\n",
    "files = glob.glob('*60mA*') + glob.glob('baseline*')\n",
    "\n",
    "# Sample numbers\n",
    "l_i = 0\n",
    "r_i = 0\n",
    "no_i = 0\n",
    "for filename in files:\n",
    "    data = load_data(filename)\n",
    "    \n",
    "    # If data is from left condition, place in first condition index of X\n",
    "    if '_l_' in filename:\n",
    "        X[:, :, l_i, L_COND_IDX] = data\n",
    "        l_i += 1    # increment left sample number\n",
    "        \n",
    "    # If data is from right condition, place in second condition index of X\n",
    "    elif '_r_' in filename:\n",
    "        X[:, :, r_i, R_COND_IDX] = data\n",
    "        r_i += 1    # incremenet right sample number\n",
    "        \n",
    "    # If data is no stimulus condition, place in third condition index of X\n",
    "    else:\n",
    "        # Once condition 3 has 21 total samples, starting addition to condition 4\n",
    "        if no_i <= (N_SAMPLES - IDX_SHIFT):\n",
    "            X[:, :, no_i, NO_COND_IDX_1] = data\n",
    "            no_i += 1    # increment no stim sample number\n",
    "        else:\n",
    "            X[:, :, no_i%(N_SAMPLES), NO_COND_IDX] = data\n",
    "            no_i += 1    # increment no stim sample number\n",
    "    \n",
    "print(\"All data, X:\", X.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:48:44.730376Z",
     "start_time": "2020-05-28T01:48:44.719001Z"
    }
   },
   "source": [
    "#### Reshape to 2D matrix\n",
    "- Reshape to X starts as [n_chans * n_tp x n_samples * n_conditions] = [360 * 51 x 21 * 4]\n",
    "- After reshaping, we transpose the matrix, so the data matrix is of size n x p, where n is the number of samples and p is the number of variables/observations/features.\n",
    "- We also want to center the data, so that the column/features means have been subtracted and are now equal to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:22:20.170643Z",
     "start_time": "2020-05-28T20:22:20.163428Z"
    }
   },
   "outputs": [],
   "source": [
    "def reshape(X, transpose=True, center=True):\n",
    "    '''\n",
    "    Reshape, transpose, and center data matrix for PCA\n",
    "    \n",
    "    params\n",
    "        X - data matrix\n",
    "        transpose - if X needs to be transposed\n",
    "        center - if X needs to be centers\n",
    "        \n",
    "    return\n",
    "        X - data matrix, reshaped, transposed, and centered\n",
    "    '''\n",
    "    \n",
    "    X = np.reshape(X, (N_CHANS*N_TIMEPTS, N_SAMPLES*N_CONDITIONS))\n",
    "    \n",
    "    if transpose:\n",
    "        # Want X to be samples/observations x features\n",
    "        X = X.T\n",
    "        \n",
    "    if center:\n",
    "        # Remove column means\n",
    "        X = X - np.mean(X, axis=0)\n",
    "        \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:22:21.561742Z",
     "start_time": "2020-05-28T20:22:21.543738Z"
    }
   },
   "outputs": [],
   "source": [
    "X = reshape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply SVD\n",
    "SVD gives X = USV^T where\n",
    "U \n",
    "- [N_CHANS*N_TIMEPTS x N_CHANS*N_TIMEPTs (N_PCs)]\n",
    "- Singular matrix containing orthogonal vectors of unit length in its row\n",
    "    \n",
    "S \n",
    "- [N_SAMPLES*N_CONDITIONS x 1]\n",
    "V\n",
    "- [N_SAMPLES*N_CONDITIONS x N_SAMPLES*N_CONDITIONS]\n",
    "- Singular matrix containing orthogonal vectors of unit length in its column\n",
    "\n",
    "V^T is the PC directions\n",
    "US is the weights/loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:27:18.832118Z",
     "start_time": "2020-05-28T02:27:18.786367Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Compute SVD of the original matrix'''\n",
    "# Keep eigenvectors that have the top kth highest singular value (PCA)\n",
    "u, s, vh = np.linalg.svd(X, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:27:20.108498Z",
     "start_time": "2020-05-28T02:27:20.104723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 63)\n",
      "(63,)\n",
      "(63, 18360)\n"
     ]
    }
   ],
   "source": [
    "print(u.shape)\n",
    "print(s.shape)\n",
    "print(vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:16:06.684140Z",
     "start_time": "2020-05-28T02:16:06.679491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.16525547e-04, -4.31302146e-03,  3.81586738e-02, ...,\n",
       "        -3.90897452e-02, -3.53526474e-02,  0.00000000e+00],\n",
       "       [ 7.70798081e-04,  2.44676074e-02,  7.76103725e-02, ...,\n",
       "        -1.40174834e-01,  4.34957876e-02,  0.00000000e+00],\n",
       "       [ 8.15456195e-05,  1.41462006e-02,  1.40036837e-02, ...,\n",
       "        -4.23021752e-02, -4.54717405e-02,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-7.36374073e-05, -1.18223929e-02,  3.25543622e-02, ...,\n",
       "        -2.62348791e-02, -9.86593758e-03,  0.00000000e+00],\n",
       "       [ 8.41505831e-04,  8.51207465e-04,  3.45390859e-02, ...,\n",
       "         1.52369890e-02, -3.53485195e-02,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T02:50:16.566109Z",
     "start_time": "2020-05-28T02:50:16.523893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18360, 63)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(63,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "print(X.T.shape)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X.T)\n",
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:58:22.902195Z",
     "start_time": "2020-05-26T20:58:22.897678Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def run_pca(data):\n",
    "    '''\n",
    "    Does PCA on normalized data set. Can optionally set fewer components\n",
    "    \n",
    "    param: data - normalized data (n_samples, n_features)\n",
    "    return: \n",
    "        data_pc: data transformed onto princinpal components\n",
    "        components:  principal axes in feature space, array, shape (n_components, n_features)\n",
    "        weights: percentage of variance explained by each of the selected components. array, shape (n_components,)\n",
    "    '''\n",
    "    # create PCA model\n",
    "    pca = PCA() \n",
    "    \n",
    "    # fit model to data\n",
    "    data_pc = pca.fit(data)  \n",
    "    \n",
    "    # obtain components and components' weights\n",
    "    components = pca.components_\n",
    "    weights = pca.explained_variance_ratio_\n",
    "    \n",
    "    return data_pc, components, weights, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:58:43.897245Z",
     "start_time": "2020-05-26T20:58:43.890465Z"
    }
   },
   "outputs": [],
   "source": [
    "def cum_var_plot(weights, desired_var, plot=False):\n",
    "    '''\n",
    "    Cumulative variance plot (number of components vs cumulative variance captured) with calculated number \n",
    "    of PCs required to get to a certain desired variance explained\n",
    "    \n",
    "    params:\n",
    "        weights: percentage of variance explained by each of the selected components. array, shape (n_components,)\n",
    "        desired_var:  percent variance to find number of PCs for\n",
    "    return\n",
    "        pcs_req:   pcs required to captured at least desired variance\n",
    "        captured_var   exact variance captured by pcs_req\n",
    "    \n",
    "    '''\n",
    "    INDEX_SHIFT = 1\n",
    "    # cumulative variance captured\n",
    "    cum_var = np.cumsum(weights) \n",
    "    \n",
    "    # find pcs req to get desired variance\n",
    "    pcs_req = int(round(np.min(np.where(cum_var > desired_var)) + 1)) \n",
    "    \n",
    "    # actual variance captured\n",
    "    captured_var = cum_var[pcs_req-INDEX_SHIFT]\n",
    "    \n",
    "    # plotting\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(range(INDEX_SHIFT,len(cum_var)+INDEX_SHIFT), cum_var)\n",
    "        plt.axvline(x=pcs_req, ymin=0, ymax=1, color='k', linestyle='--')\n",
    "        plt.xlabel('Number of components')\n",
    "        plt.ylabel('Cumulative variance captured')\n",
    "        plt.title('Cumulative Variance Captured by Principal Components')\n",
    "    \n",
    "    return pcs_req, captured_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:58:44.495305Z",
     "start_time": "2020-05-26T20:58:44.487975Z"
    }
   },
   "outputs": [],
   "source": [
    "def biplot(data, pca, plot=False):\n",
    "    '''\n",
    "    Creates biplot for data mapping data onto top 2 or 3 principal components\n",
    "    \n",
    "    params\n",
    "        data - normalized data, array shape (n_samples, n_features)\n",
    "        pca - pca model\n",
    "        \n",
    "    return\n",
    "        top_pcs[:,PC1_IDX] - pcs captured by top PC\n",
    "        top_var[PC1_IDX] - variance captured by top PC\n",
    "    '''\n",
    "    PC1_IDX = 0    # index of first PC\n",
    "    PC2_IDX = 1    # index of second PC\n",
    "    DIM = 2        # dimensions to plot \n",
    "    \n",
    "    # top 2 pc's and their variance explained\n",
    "    top_pcs = pca.transform(data)[:,:DIM]\n",
    "    top_var = pca.explained_variance_ratio_[:DIM]\n",
    "    \n",
    "    # plotting\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.scatter(top_pcs[:,PC1_IDX], top_pcs[:,PC2_IDX])\n",
    "        plt.xlabel('PC1 ({:.1%})'.format(top_var[PC1_IDX]))\n",
    "        plt.ylabel('PC2 ({:.1%})'.format(top_var[PC2_IDX]))\n",
    "        plt.axvline(x=0, ymin=np.min(top_pcs[:,PC2_IDX]), ymax=np.max(top_pcs[:,PC2_IDX]), color='k', ls='--')\n",
    "        plt.axhline(y=0, xmin=np.min(top_pcs[:,PC1_IDX]), xmax=np.max(top_pcs[:,PC1_IDX]), color='k', ls='--')\n",
    "    \n",
    "    return top_pcs[:,PC1_IDX], top_var[PC1_IDX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LDA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:18:25.148323Z",
     "start_time": "2020-05-26T23:18:25.141383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "def classify(data, clf):\n",
    "    '''\n",
    "    General classifier for reduced input data into \n",
    "    3 stimulus conditions (left, right, no stimulus)\n",
    "    \n",
    "    param:\n",
    "        data - normalized and potentially transformed to pcs (n_samples, n_features)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    N_TRIALS = 21\n",
    "    labels = np.array(['left', 'right', 'none'])\n",
    "    labels = np.repeat(labels, N_TRIALS)\n",
    "    labels = labels[:62]\n",
    "    \n",
    "    # TRANSFORM DATA TO LOWER DIM\n",
    "    # Automatically sets test as 0.25\n",
    "    # Random state so that it's the same every run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "    # train LDA classifier\n",
    "    # look into using LDA for dimensionality reduction??\n",
    "    model = clf.fit(X_train , y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    \n",
    "    # model accuracy for X_test\n",
    "    acc = 100*accuracy_score(y_test, y_predict)\n",
    "    print('Accuracy:',round(lda_acc,2),'%')\n",
    "    print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "    \n",
    "    # creating a confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Receiver Operating Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ROC curves have true positive (predicted positive and actually positive) rate on the Y axis and false positive (predicted positive, actually negative) on the X axis. THe top left corner for the plot is the \"ideal\" point with a false positive rate of zero and a true positive rate of one.  \n",
    "  \n",
    "In order to extend ROC curve and ROC area to multi-label classification, it is necessary to binarize the output (one vs all). One ROC curve can be drawn per label and/or one can draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).  \n",
    "  \n",
    "Another evaluation for multi-label classification is macro-averaging, which gives equal weight to the classification of each label.  \n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#plot-roc-curves-for-the-multilabel-problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def roc_curve():\n",
    "    #https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "    #https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier/56092736\n",
    "    #https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#in-multi-label-settings\n",
    "    \n",
    "    # Precision-Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TODO: Add regularization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TODO: Add bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:39:24.971718Z",
     "start_time": "2020-05-26T22:39:15.928146Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Mounted shared Google Drive for data\n",
    "Data is recordings from 360 good channels x time\n",
    "Data should be samples x features (time x features)\n",
    "360 features, 51 samples. '''\n",
    "'''21 right, 21 left, 20 baseline'''\n",
    "N_SAMPLES = 62\n",
    "N_TIMEPOINTS = 51\n",
    "N_EXTRA_DATA = 2\n",
    "\n",
    "'''\n",
    "Output is top pc for all time points (0:50), the variance explained by\n",
    "top output(51), and the total number of pc's required for 80% variance\n",
    "explained by number of samples (21 left, 21 right)'''\n",
    "output = np.zeros([N_TIMEPOINTS + N_EXTRA_DATA, N_SAMPLES])\n",
    "\n",
    "# 60mA stim and baseline\n",
    "os.chdir('/Volumes/GoogleDrive/Shared drives/COGS 260 Project/Data/fragments3')\n",
    "files = glob.glob('*60mA*') + glob.glob('baseline*')\n",
    "\n",
    "trial_names = []    # initialize list of trials\n",
    "index = 0      # initialize index\n",
    "\n",
    "# Run PCA on all trials\n",
    "for filename in files:\n",
    "    if '60' in filename:\n",
    "        trial_names.append((filename.split('_')[1] + '_trial_' \n",
    "                            + filename.split('_')[5]).split('.')[0])\n",
    "    else:\n",
    "        trial_names.append(filename.split('.')[0])\n",
    "           \n",
    "    # Initialize trial results\n",
    "    trial_results = np.zeros([N_TIMEPOINTS + N_EXTRA_DATA])\n",
    "    \n",
    "    # Load and normalize data\n",
    "    data = load_data(filename, transpose=True)\n",
    "    data = norm_data(data)\n",
    "    \n",
    "    # Run pca\n",
    "    data_pc, components, weights, pca = run_pca(data)\n",
    "    \n",
    "    # further analysis\n",
    "    # plotting options turned off\n",
    "    pcs_req, captured_var = cum_var_plot(weights, 0.8)\n",
    "    top_pc, top_var = biplot(data, pca)\n",
    "\n",
    "    # Organize results for single trial\n",
    "    trial_results[:N_TIMEPOINTS] = top_pc\n",
    "    trial_results[N_TIMEPOINTS] = top_var\n",
    "    trial_results[N_TIMEPOINTS + 1] = pcs_req\n",
    "\n",
    "    # Add to all trial output\n",
    "    output[:, index] = trial_results\n",
    "\n",
    "    # incremement index\n",
    "    index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output, columns=trial_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is now timepoints x trials, and we are interested in using the timepoints as the features, so the data must be transformed. The data has to also be renormalized, so the mean and standard deviation for each timepoint feature is 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T02:02:02.832680Z",
     "start_time": "2020-05-27T02:02:02.808643Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-337-25dfde597d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN_TIMEPOINTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorr_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-336-443fc69ef19e>\u001b[0m in \u001b[0;36mcorr_heatmap\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorr_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     ax = sns.heatmap(\n\u001b[1;32m      4\u001b[0m         \u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'corr'"
     ]
    }
   ],
   "source": [
    "data = output[:N_TIMEPOINTS,:]\n",
    "data = norm_data(data.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(data, LinearDiscriminantAnalysis())\n",
    "classify(data, DecisionTreeClassifier(class_weight='balanced'))\n",
    "classify(data, KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:19:43.581869Z",
     "start_time": "2020-05-26T23:19:43.567267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.5 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.40      0.50      0.44         4\n",
      "       Right       0.75      0.75      0.75         4\n",
      "        None       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.62      0.62      0.62        16\n",
      "weighted avg       0.64      0.62      0.63        16\n",
      "\n",
      "[[2 0 2]\n",
      " [1 3 0]\n",
      " [2 1 5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "N_TRIALS = 21\n",
    "labels = np.array(['left', 'right', 'none'])\n",
    "labels = np.repeat(labels, N_TRIALS)\n",
    "labels = labels[:62]\n",
    "\n",
    "# TRANSFORM DATA TO LOWER DIM\n",
    "# Automatically sets test as 0.25\n",
    "# Random state so that it's the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "# train DTree\n",
    "model = DecisionTreeClassifier(class_weight='balanced')\n",
    "model.fit(X=X_train,y=y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# model accuracy for X_test\n",
    "acc = 100*accuracy_score(y_test, y_predict)\n",
    "print('Accuracy:',round(acc,2),'%')\n",
    "print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:20:05.468757Z",
     "start_time": "2020-05-26T23:20:05.457480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.75 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.40      0.50      0.44         4\n",
      "       Right       0.33      0.25      0.29         4\n",
      "        None       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.56        16\n",
      "   macro avg       0.49      0.50      0.49        16\n",
      "weighted avg       0.56      0.56      0.56        16\n",
      "\n",
      "[[2 1 1]\n",
      " [2 1 1]\n",
      " [1 1 6]]\n"
     ]
    }
   ],
   "source": [
    "classify(data, DecisionTreeClassifier(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:21:44.828601Z",
     "start_time": "2020-05-26T23:21:44.814642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.0 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.29      0.50      0.36         4\n",
      "       Right       0.50      0.50      0.50         4\n",
      "        None       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.53      0.50      0.49        16\n",
      "weighted avg       0.60      0.50      0.52        16\n",
      "\n",
      "[[2 1 1]\n",
      " [2 2 0]\n",
      " [3 1 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# TRANSFORM DATA TO LOWER DIM\n",
    "# Automatically sets test as 0.25\n",
    "# Random state so that it's the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "# train SVM classifier\n",
    "model = SVC(kernel = 'linear', class_weight='balanced').fit(X_train,y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# model accuracy for X_test\n",
    "acc = 100*accuracy_score(y_test, y_predict)\n",
    "print('Accuracy:',round(acc,2),'%')\n",
    "print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:21:50.708244Z",
     "start_time": "2020-05-26T23:21:50.693744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.5 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.25      0.25      0.25         4\n",
      "       Right       0.60      0.75      0.67         4\n",
      "        None       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.57      0.58      0.57        16\n",
      "weighted avg       0.64      0.62      0.63        16\n",
      "\n",
      "[[1 2 1]\n",
      " [1 3 0]\n",
      " [2 0 6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# TRANSFORM DATA TO LOWER DIM\n",
    "# Automatically sets test as 0.25\n",
    "# Random state so that it's the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "# train SVM classifier\n",
    "model = KNeighborsClassifier().fit(X_train,y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# model accuracy for X_test\n",
    "acc = 100*accuracy_score(y_test, y_predict)\n",
    "print('Accuracy:',round(acc,2),'%')\n",
    "print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T23:21:59.031696Z",
     "start_time": "2020-05-26T23:21:59.017950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.0 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.50      0.50      0.50         4\n",
      "       Right       0.67      1.00      0.80         4\n",
      "        None       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.72      0.75      0.72        16\n",
      "weighted avg       0.79      0.75      0.75        16\n",
      "\n",
      "[[2 2 0]\n",
      " [0 4 0]\n",
      " [2 0 6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# TRANSFORM DATA TO LOWER DIM\n",
    "# Automatically sets test as 0.25\n",
    "# Random state so that it's the same every run\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "# train SVM classifier\n",
    "model = GaussianNB().fit(X_train,y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# model accuracy for X_test\n",
    "acc = 100*accuracy_score(y_test, y_predict)\n",
    "print('Accuracy:',round(acc,2),'%')\n",
    "print(classification_report(y_test, y_predict,target_names=['Left', 'Right', 'None']))\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clean up classifier function to generalize\n",
    "2. Add bootstrapping to classifier function\n",
    "3. Look at stim vs no stim separately from left vs right\n",
    "4. Keep these results to show justification for splitting it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
